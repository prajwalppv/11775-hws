{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import _pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DIR = \"features\"\n",
    "FUSION_DIR = \"features/double_fusion\"\n",
    "with open(\"list/all.video\") as f:\n",
    "    all_videos = [l.strip() for l in f.readlines()]\n",
    "\n",
    "with open('list/train.video') as f:\n",
    "    train_videos = [l.strip() for l in f.readlines()]\n",
    "    \n",
    "with open('list/val.video') as f:\n",
    "    val_videos = [l.strip() for l in f.readlines()]\n",
    "\n",
    "with open('../all_test.video') as f:\n",
    "    test_videos = [l.strip() for l in f.readlines()]\n",
    "    \n",
    "# train labels\n",
    "train_labels = {}\n",
    "with open(\"../all_trn.lst\") as f:\n",
    "    for line in f.readlines():\n",
    "        file, label = line.strip().split()\n",
    "        train_labels[file] = label\n",
    "        \n",
    "# val labels\n",
    "val_labels = {}\n",
    "with open(\"../all_val.lst\") as f:\n",
    "    for line in f.readlines():\n",
    "        file, label = line.strip().split()\n",
    "        val_labels[file] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 836/836 [00:01<00:00, 704.27it/s]\n",
      "100%|██████████| 836/836 [00:00<00:00, 1175.18it/s]\n",
      "100%|██████████| 836/836 [00:01<00:00, 735.70it/s]\n",
      "100%|██████████| 836/836 [00:00<00:00, 665358.28it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 665.87it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 1153.85it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 789.20it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 452826.34it/s]\n",
      "100%|██████████| 1699/1699 [00:02<00:00, 708.30it/s]\n",
      "100%|██████████| 1699/1699 [00:01<00:00, 1192.39it/s]\n",
      "100%|██████████| 1699/1699 [00:02<00:00, 793.51it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_features(video_list,feature_type):\n",
    "    features = [] \n",
    "    for file in tqdm(video_list):\n",
    "        if feature_type == 'soundnet':\n",
    "            feature_file = os.path.join(FEATURE_DIR,\"soundnet\",file+\".feats\")\n",
    "            with open(feature_file,\"rb\") as f:\n",
    "                features.append(np.loadtxt(f,delimiter=';'))\n",
    "        elif feature_type == 'resnet50':\n",
    "            feature_file = os.path.join(FEATURE_DIR,\"resnet50\",file+\".npy\")   \n",
    "            with open(feature_file,\"rb\") as f:\n",
    "                features.append(np.load(f))\n",
    "        else:\n",
    "            feature_file = os.path.join(FEATURE_DIR,\"places\",file+\".npy\")   \n",
    "            with open(feature_file,\"rb\") as f:\n",
    "                features.append(np.load(f))\n",
    "                \n",
    "    features = np.vstack(features)\n",
    "    return features\n",
    "\n",
    "def load_labels(video_list,label_map):\n",
    "    labels = []\n",
    "    for video in tqdm(video_list):\n",
    "        labels.append(label_map[video])\n",
    "        \n",
    "    return np.array(labels)\n",
    "\n",
    "train_soundnet_features = load_features(train_videos,'soundnet')\n",
    "train_resnet_features = load_features(train_videos,'resnet50')\n",
    "train_places_features = load_features(train_videos,'places')\n",
    "\n",
    "train_y = load_labels(train_videos,train_labels)\n",
    "\n",
    "val_soundnet_features = load_features(val_videos,'soundnet')\n",
    "val_resnet_features = load_features(val_videos,'resnet50')\n",
    "val_places_features = load_features(val_videos,'places')\n",
    "\n",
    "val_y = load_labels(val_videos,val_labels)\n",
    "\n",
    "test_soundnet_features = load_features(test_videos,'soundnet')\n",
    "test_resnet_features = load_features(test_videos,'resnet50')\n",
    "test_places_features = load_features(test_videos,'places')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairwise_features(soundnet, resnet, places):\n",
    "    sound_res = np.hstack((soundnet,resnet))\n",
    "    sound_places = np.hstack((soundnet,places))\n",
    "    res_places = np.hstack((resnet,places))\n",
    "    all_comb = np.hstack((soundnet,resnet,places))\n",
    "    \n",
    "#     return [sound_res, sound_places, res_places, all_comb]\n",
    "#     return [sound_res, res_places]\n",
    "    return sound_res,res_places\n",
    "\n",
    "train_SR_features,train_RP_features = generate_pairwise_features(train_soundnet_features,train_resnet_features,train_places_features)\n",
    "val_SR_features,val_RP_features = generate_pairwise_features(val_soundnet_features,val_resnet_features,val_places_features)\n",
    "test_SR_features, test_RP_features = generate_pairwise_features(test_soundnet_features,test_resnet_features,test_places_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling complete\n"
     ]
    }
   ],
   "source": [
    "def downsample_data(train_x,train_y,val_x,val_y,combine):\n",
    "#     print(\"Downsampling\")\n",
    "# Combine train and validation set into a single train dataset\n",
    "    if combine:\n",
    "        all_train_x = np.vstack([train_x,val_x])\n",
    "        all_train_y = np.concatenate((train_y,val_y))\n",
    "\n",
    "        # Shuffle dataset\n",
    "        idxs = [i for i in range(all_train_x.shape[0])]\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        all_train_x = all_train_x[idxs]\n",
    "        all_train_y = all_train_y[idxs]\n",
    "    else:\n",
    "        all_train_x = train_x\n",
    "        all_train_y = train_y\n",
    "\n",
    "    positive_examples = all_train_x[all_train_y!='NULL']\n",
    "    negative_examples = all_train_x[all_train_y=='NULL']\n",
    "\n",
    "    positive_labels = all_train_y[all_train_y!='NULL']\n",
    "    negative_labels = all_train_y[all_train_y == 'NULL']\n",
    "\n",
    "    np.random.shuffle(negative_examples)\n",
    "    negative_examples = negative_examples[:len(positive_labels)//2]\n",
    "    negative_labels = negative_labels[:len(positive_labels)//2]\n",
    "\n",
    "    combined_data = np.vstack((positive_examples,negative_examples))\n",
    "    combined_labels = np.concatenate((positive_labels,negative_labels))\n",
    "    \n",
    "    return combined_data,combined_labels\n",
    "\n",
    "COMBINE = False\n",
    "soundnet_x,soundnet_y = downsample_data(train_soundnet_features,train_y,val_soundnet_features,val_y,combine=COMBINE)\n",
    "resnet_x, resnet_y = downsample_data(train_resnet_features,train_y,val_resnet_features,val_y,combine=COMBINE)\n",
    "places_x, places_y = downsample_data(train_places_features,train_y,val_places_features,val_y,combine=COMBINE)\n",
    "sr_x, sr_y = downsample_data(train_SR_features,train_y,val_SR_features,val_y,combine=COMBINE)\n",
    "rp_x, rp_y = downsample_data(train_RP_features,train_y,val_RP_features,val_y,combine=COMBINE)\n",
    "\n",
    "idxs = [i for i in range(len(soundnet_x))]\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "soundnet_x = soundnet_x[idxs]\n",
    "soundnet_y = soundnet_y[idxs]\n",
    "\n",
    "resnet_x = resnet_x[idxs]\n",
    "resnet_y = resnet_y[idxs]\n",
    "\n",
    "places_x = places_x[idxs]\n",
    "places_y = places_y[idxs]\n",
    "\n",
    "sr_x = sr_x[idxs]\n",
    "sr_y = sr_y[idxs]\n",
    "\n",
    "rp_x = rp_x[idxs]\n",
    "rp_y = rp_y[idxs]\n",
    "\n",
    "print(\"Downsampling complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.02292690350559003 \n",
      "Loss : 0.0004771475513751168 \n",
      "Loss : 0.000685666772717504 \n",
      "Loss : 0.0005841141290220448 \n",
      "Loss : 0.0007679877114349029 \n"
     ]
    }
   ],
   "source": [
    "MODEL_USED = 'mlp'\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_NAME = 'df_exp1'\n",
    "EVENTS = [\"NULL\",\"P001\",\"P002\",\"P003\"]\n",
    "EVENTS_MAP = {\"NULL\":0,\"P001\":1,\"P002\":2,\"P003\":3}\n",
    "\n",
    "def train_classifier(combined_data,combined_labels):\n",
    "# Train 3 separate models for each event\n",
    "# MLP parameters\n",
    "\n",
    "\n",
    "    hidden_layers = (1024,2048)\n",
    "    lr = 1e-3\n",
    "    batch_size = 50\n",
    "    alpha = 1e-5\n",
    "    loss = []\n",
    "\n",
    "\n",
    "    # Random Forest params\n",
    "    num_estimators = 50\n",
    "    max_depth = 10\n",
    "    event_y = np.array([EVENTS_MAP[k] for k in combined_labels])\n",
    "    \n",
    "    if MODEL_USED == 'rf':\n",
    "        model = RandomForestClassifier(n_estimators=num_estimators,max_depth=max_depth)\n",
    "#         model = GradientBoostingClassifier(n_estimators=num_estimators,max_depth=max_depth)\n",
    "    elif MODEL_USED == 'mlp':\n",
    "        model = MLPClassifier(hidden_layer_sizes=hidden_layers,\n",
    "                         alpha=alpha,\n",
    "                         batch_size=batch_size,\n",
    "                         learning_rate_init=lr,\n",
    "                         max_iter=2000,\n",
    "                            verbose=False,\n",
    "                         )\n",
    "    model.fit(combined_data,event_y)\n",
    "    if MODEL_USED== 'mlp':\n",
    "        loss.append(model.loss_)\n",
    "        print(\"Loss : {} \".format(model.loss_))\n",
    "        \n",
    "    return model\n",
    "\n",
    "soundnet_classifier = train_classifier(soundnet_x,soundnet_y)\n",
    "resnet_classifier = train_classifier(resnet_x,resnet_y)\n",
    "places_classifier = train_classifier(places_x,places_y)\n",
    "sr_classifier = train_classifier(sr_x,sr_y)\n",
    "rp_classifier = train_classifier(rp_x,rp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundnet_post_features = soundnet_classifier.predict_log_proba(soundnet_x)\n",
    "resnet_post_features = resnet_classifier.predict_log_proba(resnet_x)\n",
    "places_post_features = places_classifier.predict_log_proba(places_x)\n",
    "sr_post_features = sr_classifier.predict_log_proba(sr_x)\n",
    "rp_post_features = rp_classifier.predict_log_proba(rp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_features = np.hstack((soundnet_post_features,resnet_post_features,\n",
    "                    places_post_features,sr_post_features,rp_post_features))\n",
    "post_labels = soundnet_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:00<00:00, 11.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event NULL loss : 0.00033421912005690086 \n",
      "Event P001 loss : 9.780088712169413e-05 \n",
      "Event P002 loss : 5.663116167557355e-06 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 4/4 [00:00<00:00, 12.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event P003 loss : 0.0034238954349864637 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train 3 separate models for each event\n",
    "# MLP parameters\n",
    "MODEL_USED = 'mlp'\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_NAME = 'lf_exp1'\n",
    "EVENTS = [\"NULL\",\"P001\",\"P002\",\"P003\"]\n",
    "\n",
    "hidden_layers = (512)\n",
    "lr = 1e-3\n",
    "batch_size = 100\n",
    "alpha = 1e-5\n",
    "loss = []\n",
    "\n",
    "\n",
    "# Random Forest params\n",
    "num_estimators = 3000\n",
    "max_depth = 30\n",
    "\n",
    "for event in tqdm(EVENTS[:]):\n",
    "    # Train for event\n",
    "    event_y = (post_labels==event).astype('int')\n",
    "    if MODEL_USED == 'rf':\n",
    "        model = RandomForestClassifier(n_estimators=num_estimators,max_depth=max_depth)\n",
    "#         model = GradientBoostingClassifier(n_estimators=num_estimators,max_depth=max_depth)\n",
    "    elif MODEL_USED == 'mlp':\n",
    "        model = MLPClassifier(hidden_layer_sizes=hidden_layers,\n",
    "                         alpha=alpha,\n",
    "                         batch_size=batch_size,\n",
    "                         learning_rate_init=lr,\n",
    "                         max_iter=2000,\n",
    "                            verbose=False,\n",
    "                         )\n",
    "    model.fit(post_features,event_y)\n",
    "    if MODEL_USED== 'mlp':\n",
    "        loss.append(model.loss_)\n",
    "        print(\"Event {} loss : {} \".format(event,model.loss_))\n",
    "    # Save model\n",
    "    model_path = os.path.join(MODEL_DIR,MODEL_NAME+\"_\" +event+\".model\")\n",
    "    with open(model_path,\"wb\") as o:\n",
    "        pkl.dump(model,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1699/1699 [00:00<00:00, 342355.15it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 325265.92it/s]\n",
      "100%|██████████| 1699/1699 [00:00<00:00, 320145.67it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 301477.38it/s]\n",
      "100%|██████████| 1699/1699 [00:00<00:00, 295285.40it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 285278.29it/s]\n",
      "100%|██████████| 1699/1699 [00:00<00:00, 311988.20it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 309086.51it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 122910.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event NULL val accuracy : 0.65\n",
      "Event P001 val accuracy : 0.65\n",
      "Event P002 val accuracy : 0.87\n",
      "Event P003 val accuracy : 0.89\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PRED_DIR = 'pred/DF'\n",
    "MODE = 'DF'\n",
    "\n",
    "for event in EVENTS[:]:\n",
    "    # Load model for event and predict\n",
    "    model_path = os.path.join(MODEL_DIR,MODEL_NAME+\"_\" +event+\".model\")\n",
    "    with open(model_path,\"rb\") as l:\n",
    "        model = pkl.load(l,encoding='bytes')\n",
    "        val_pred = os.path.join(PRED_DIR,MODE+\"_val_\"+event+\".csv\")\n",
    "        test_pred = os.path.join(PRED_DIR,MODE+\"_\"+event+\".csv\")\n",
    "        \n",
    "        with open(test_pred,\"w\") as o: \n",
    "            test_soundnet_post_features = soundnet_classifier.predict_log_proba(test_soundnet_features)\n",
    "            test_resnet_post_features = resnet_classifier.predict_log_proba(test_resnet_features)\n",
    "            test_places_post_features = places_classifier.predict_log_proba(test_places_features)\n",
    "            test_sr_post_features = sr_classifier.predict_log_proba(test_SR_features)\n",
    "            test_rp_post_features = rp_classifier.predict_log_proba(test_RP_features)\n",
    "            \n",
    "            test_post_features = np.hstack((test_soundnet_post_features,test_resnet_post_features,\n",
    "                                            test_places_post_features,test_sr_post_features,test_rp_post_features))\n",
    "            \n",
    "            prediction = model.predict_proba(test_post_features)\n",
    "            for p in tqdm(prediction):\n",
    "#                 print(p)\n",
    "                o.write(\"{}\\n\".format(p[1]))\n",
    "#           prediction = model.decision_function(feat)\n",
    "#           o.write(\"{}\\n\".format(prediction[0]))\n",
    "\n",
    "\n",
    "                    \n",
    "        with open(val_pred,\"w\") as o: \n",
    "            val_soundnet_post_features = soundnet_classifier.predict_log_proba(val_soundnet_features)\n",
    "            val_resnet_post_features = resnet_classifier.predict_log_proba(val_resnet_features)\n",
    "            val_places_post_features = places_classifier.predict_log_proba(val_places_features)\n",
    "            val_sr_post_features = sr_classifier.predict_log_proba(val_SR_features)\n",
    "            val_rp_post_features = rp_classifier.predict_log_proba(val_RP_features)\n",
    "            \n",
    "            val_post_features = np.hstack((val_soundnet_post_features,val_resnet_post_features,\n",
    "                                           val_places_post_features,val_sr_post_features,val_rp_post_features))\n",
    "            \n",
    "            prediction = model.predict_proba(val_post_features)\n",
    "            for p in tqdm(prediction):\n",
    "                o.write(\"{}\\n\".format(p[1]))\n",
    "#               prediction = model.decision_function(feat)\n",
    "#               o.write(\"{}\\n\".format(prediction[0]))\n",
    "                     \n",
    "p0 = open(PRED_DIR + \"/\" + MODE + \"_val_NULL.csv\").readlines()                    \n",
    "p1 = open(PRED_DIR + \"/\" + MODE + \"_val_P001.csv\").readlines()\n",
    "p2 = open(PRED_DIR + \"/\" + MODE + \"_val_P002.csv\").readlines()\n",
    "p3 = open(PRED_DIR + \"/\" + MODE + \"_val_P003.csv\").readlines()\n",
    "\n",
    "p0 = [float(p.strip()) for p in p0]\n",
    "p1 = [float(p.strip()) for p in p1]\n",
    "p2 = [float(p.strip()) for p in p2]\n",
    "p3 = [float(p.strip()) for p in p3]\n",
    "\n",
    "acc = [0,0,0,0]\n",
    "total = [0,0,0,0]\n",
    "\n",
    "THRESHOLD = 0\n",
    "correct = 0\n",
    "with open(\"../all_val.lst\") as f:\n",
    "    lines = f.readlines()\n",
    "    for idx,line in enumerate(tqdm(lines)):\n",
    "        filename, truth = line.strip().split()\n",
    "        label = [p0[idx],p1[idx],p2[idx],p3[idx]]\n",
    "        pred = np.argmax(label)\n",
    "        \n",
    "        if EVENTS[pred] == truth:\n",
    "            acc[pred] += 1\n",
    "        total[EVENTS.index(truth)] += 1\n",
    "            \n",
    "acc = np.array(acc)/np.array(total)\n",
    "print(\"Event NULL val accuracy : {0:1.2f}\\nEvent P001 val accuracy : {0:1.2f}\\nEvent P002 val accuracy : {1:1.2f}\\nEvent P003 val accuracy : {2:1.2f}\\n\"\n",
    "      .format(acc[0],acc[1],acc[2],acc[3]))\n",
    "\n",
    "# Write params in log\n",
    "with open(\"logs.txt\",\"a\") as log:\n",
    "    if MODEL_USED == 'mlp':\n",
    "        log.write(\"{0}#{1}#{2}#{3}#{4}#{5:0.2f}#{6:0.2f}#{7:0.2f}#{8:0.2f}#{9:0.2f}#{10:0.2f}\\n\".format(MODEL_USED,hidden_layers,lr,batch_size,\n",
    "                                                       alpha,loss[0],loss[1],loss[2],acc[0],acc[1],acc[2]))\n",
    "    elif MODEL_USED == 'rf':\n",
    "        log.write(\"{0}#{1}#{2}#{3:0.2f}#{4:0.2f}#{5:0.2f}\\n\".format(MODEL_USED,num_estimators,max_depth,acc[0],acc[1],acc[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the average precision (AP)\n",
      "Average precision:  0.7989637360007185\n",
      "Evaluating the average precision (AP)\n",
      "Average precision:  0.9682407407407407\n",
      "Evaluating the average precision (AP)\n",
      "Average precision:  0.6738261550999769\n"
     ]
    }
   ],
   "source": [
    "!python eval.py list/P001_val_label pred/DF/DF_val_P001.csv\n",
    "!python eval.py list/P002_val_label pred/DF/DF_val_P002.csv\n",
    "!python eval.py list/P003_val_label pred/DF/DF_val_P003.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1699/1699 [00:00<00:00, 113143.59it/s]\n"
     ]
    }
   ],
   "source": [
    "p0 = open(PRED_DIR + \"/\" + MODE + \"_NULL.csv\").readlines()\n",
    "p1 = open(PRED_DIR + \"/\" + MODE + \"_P001.csv\").readlines()\n",
    "p2 = open(PRED_DIR + \"/\" + MODE + \"_P002.csv\").readlines()\n",
    "p3 = open(PRED_DIR + \"/\" + MODE + \"_P003.csv\").readlines()\n",
    "\n",
    "p0 = [float(p.strip())-0.3 for p in p0]\n",
    "p1 = [float(p.strip()) for p in p1]\n",
    "p2 = [float(p.strip()) for p in p2]\n",
    "p3 = [float(p.strip()) for p in p3]\n",
    "\n",
    "THRESHOLD = 0\n",
    "with open((MODE+\"_kaggle_prediction.csv\"),\"w\") as o:\n",
    "    o.write(\"VideoID,Label\\n\")\n",
    "    for idx,video in enumerate(tqdm(test_videos)):\n",
    "        label = [p0[idx],p1[idx],p2[idx],p3[idx]]\n",
    "        idx = np.argmax(label)\n",
    "        pred = idx\n",
    "        o.write(\"{},{}\\n\".format(video,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
