{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import _pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DIR = \"features\"\n",
    "FUSION_DIR = \"features/late_fusion\"\n",
    "with open(\"list/all.video\") as f:\n",
    "    all_videos = [l.strip() for l in f.readlines()]\n",
    "\n",
    "with open('list/train.video') as f:\n",
    "    train_videos = [l.strip() for l in f.readlines()]\n",
    "    \n",
    "with open('list/val.video') as f:\n",
    "    val_videos = [l.strip() for l in f.readlines()]\n",
    "\n",
    "with open('../all_test.video') as f:\n",
    "    test_videos = [l.strip() for l in f.readlines()]\n",
    "    \n",
    "# train labels\n",
    "train_labels = {}\n",
    "with open(\"../all_trn.lst\") as f:\n",
    "    for line in f.readlines():\n",
    "        file, label = line.strip().split()\n",
    "        train_labels[file] = label\n",
    "        \n",
    "# val labels\n",
    "val_labels = {}\n",
    "with open(\"../all_val.lst\") as f:\n",
    "    for line in f.readlines():\n",
    "        file, label = line.strip().split()\n",
    "        val_labels[file] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate individual features and train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 836/836 [00:00<00:00, 1049.74it/s]\n",
      "100%|██████████| 836/836 [00:00<00:00, 3271.30it/s]\n",
      "100%|██████████| 836/836 [00:00<00:00, 3240.58it/s]\n",
      "100%|██████████| 836/836 [00:00<00:00, 766267.08it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 1030.24it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 3240.33it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 3272.84it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 282730.30it/s]\n",
      "100%|██████████| 1699/1699 [00:01<00:00, 1049.17it/s]\n",
      "100%|██████████| 1699/1699 [00:00<00:00, 3169.17it/s]\n",
      "100%|██████████| 1699/1699 [00:00<00:00, 3240.69it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_features(video_list,feature_type):\n",
    "    features = [] \n",
    "    for file in tqdm(video_list):\n",
    "        if feature_type == 'soundnet':\n",
    "            feature_file = os.path.join(FEATURE_DIR,\"soundnet\",file+\".feats\")\n",
    "            with open(feature_file,\"rb\") as f:\n",
    "                features.append(np.loadtxt(f,delimiter=';'))\n",
    "        elif feature_type == 'resnet50':\n",
    "            feature_file = os.path.join(FEATURE_DIR,\"resnet50\",file+\".npy\")   \n",
    "            with open(feature_file,\"rb\") as f:\n",
    "                features.append(np.load(f))\n",
    "        else:\n",
    "            feature_file = os.path.join(FEATURE_DIR,\"places\",file+\".npy\")   \n",
    "            with open(feature_file,\"rb\") as f:\n",
    "                features.append(np.load(f))\n",
    "                \n",
    "    features = np.vstack(features)\n",
    "    return features\n",
    "\n",
    "def load_labels(video_list,label_map):\n",
    "    labels = []\n",
    "    for video in tqdm(video_list):\n",
    "        labels.append(label_map[video])\n",
    "        \n",
    "    return np.array(labels)\n",
    "\n",
    "train_soundnet_features = load_features(train_videos,'soundnet')\n",
    "train_resnet_features = load_features(train_videos,'resnet50')\n",
    "train_places_features = load_features(train_videos,'places')\n",
    "\n",
    "train_y = load_labels(train_videos,train_labels)\n",
    "\n",
    "val_soundnet_features = load_features(val_videos,'soundnet')\n",
    "val_resnet_features = load_features(val_videos,'resnet50')\n",
    "val_places_features = load_features(val_videos,'places')\n",
    "\n",
    "val_y = load_labels(val_videos,val_labels)\n",
    "\n",
    "test_soundnet_features = load_features(test_videos,'soundnet')\n",
    "test_resnet_features = load_features(test_videos,'resnet50')\n",
    "test_places_features = load_features(test_videos,'places')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling complete\n"
     ]
    }
   ],
   "source": [
    "def downsample_data(train_x,train_y,val_x,val_y,combine):\n",
    "#     print(\"Downsampling\")\n",
    "# Combine train and validation set into a single train dataset\n",
    "    if combine:\n",
    "        all_train_x = np.vstack([train_x,val_x])\n",
    "        all_train_y = np.concatenate((train_y,val_y))\n",
    "\n",
    "        # Shuffle dataset\n",
    "        idxs = [i for i in range(all_train_x.shape[0])]\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        all_train_x = all_train_x[idxs]\n",
    "        all_train_y = all_train_y[idxs]\n",
    "    else:\n",
    "        all_train_x = train_x\n",
    "        all_train_y = train_y\n",
    "\n",
    "    positive_examples = all_train_x[all_train_y!='NULL']\n",
    "    negative_examples = all_train_x[all_train_y=='NULL']\n",
    "\n",
    "    positive_labels = all_train_y[all_train_y!='NULL']\n",
    "    negative_labels = all_train_y[all_train_y == 'NULL']\n",
    "\n",
    "    np.random.shuffle(negative_examples)\n",
    "    negative_examples = negative_examples[:len(positive_labels)]\n",
    "    negative_labels = negative_labels[:len(positive_labels)]\n",
    "\n",
    "    combined_data = np.vstack((positive_examples,negative_examples))\n",
    "    combined_labels = np.concatenate((positive_labels,negative_labels))\n",
    "    \n",
    "    return combined_data,combined_labels\n",
    "\n",
    "COMBINE = True\n",
    "soundnet_x,soundnet_y = downsample_data(train_soundnet_features,train_y,val_soundnet_features,val_y,combine=COMBINE)\n",
    "resnet_x, resnet_y = downsample_data(train_resnet_features,train_y,val_resnet_features,val_y,combine=COMBINE)\n",
    "places_x, places_y = downsample_data(train_places_features,train_y,val_places_features,val_y,combine=COMBINE)\n",
    "\n",
    "# Shuffle the dataset\n",
    "idxs = [i for i in range(len(soundnet_x))]\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "soundnet_x = soundnet_x[idxs]\n",
    "soundnet_y = soundnet_y[idxs]\n",
    "\n",
    "resnet_x = resnet_x[idxs]\n",
    "resnet_y = resnet_y[idxs]\n",
    "\n",
    "places_x = places_x[idxs]\n",
    "places_y = places_y[idxs]\n",
    "\n",
    "print(\"Downsampling complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_USED = 'mlp'\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_NAME = 'lf_exp1'\n",
    "EVENTS = [\"NULL\",\"P001\",\"P002\",\"P003\"]\n",
    "EVENTS_MAP = {\"NULL\":0,\"P001\":1,\"P002\":2,\"P003\":3}\n",
    "\n",
    "def train_classifier(combined_data,combined_labels):\n",
    "# Train 3 separate models for each event\n",
    "# MLP parameters\n",
    "\n",
    "\n",
    "    hidden_layers = (2048,4096,2048)\n",
    "    lr = 1e-3\n",
    "    batch_size = 100\n",
    "    alpha = 1e-5\n",
    "    loss = []\n",
    "\n",
    "\n",
    "    # Random Forest params\n",
    "    num_estimators = 500\n",
    "    max_depth = 30\n",
    "    event_y = np.array([EVENTS_MAP[k] for k in combined_labels])\n",
    "    \n",
    "    if MODEL_USED == 'rf':\n",
    "        model = RandomForestClassifier(n_estimators=num_estimators,max_depth=max_depth)\n",
    "#         model = GradientBoostingClassifier(n_estimators=num_estimators,max_depth=max_depth)\n",
    "    elif MODEL_USED == 'mlp':\n",
    "        model = MLPClassifier(hidden_layer_sizes=hidden_layers,\n",
    "                         alpha=alpha,\n",
    "                         batch_size=batch_size,\n",
    "                         learning_rate_init=lr,\n",
    "                         max_iter=2000,\n",
    "                            verbose=False,\n",
    "                         )\n",
    "    model.fit(combined_data,event_y)\n",
    "    if MODEL_USED== 'mlp':\n",
    "        loss.append(model.loss_)\n",
    "        print(\"Loss : {} \".format(model.loss_))\n",
    "        \n",
    "    return model\n",
    "\n",
    "soundnet_classifier = train_classifier(soundnet_x,soundnet_y)\n",
    "resnet_classifier = train_classifier(resnet_x,resnet_y)\n",
    "places_classifier = train_classifier(places_x,places_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundnet_post_features = soundnet_classifier.predict_log_proba(soundnet_x)\n",
    "resnet_post_features = resnet_classifier.predict_log_proba(resnet_x)\n",
    "places_post_features = places_classifier.predict_log_proba(places_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_features = np.hstack((soundnet_post_features,resnet_post_features,places_post_features))\n",
    "post_labels = soundnet_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train final - late fusion classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 3 separate models for each event\n",
    "# MLP parameters\n",
    "MODEL_USED = 'mlp'\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_NAME = 'lf_exp1'\n",
    "EVENTS = [\"NULL\",\"P001\",\"P002\",\"P003\"]\n",
    "\n",
    "hidden_layers = (1024)\n",
    "lr = 1e-3\n",
    "batch_size = 100\n",
    "alpha = 1e-5\n",
    "loss = []\n",
    "\n",
    "\n",
    "# Random Forest params\n",
    "num_estimators = 500\n",
    "max_depth = 20\n",
    "\n",
    "for event in tqdm(EVENTS[:]):\n",
    "    # Train for event\n",
    "    event_y = (post_labels==event).astype('int')\n",
    "    if MODEL_USED == 'rf':\n",
    "        model = RandomForestClassifier(n_estimators=num_estimators,max_depth=max_depth)\n",
    "#         model = GradientBoostingClassifier(n_estimators=num_estimators,max_depth=max_depth)\n",
    "    elif MODEL_USED == 'mlp':\n",
    "        model = MLPClassifier(hidden_layer_sizes=hidden_layers,\n",
    "                         alpha=alpha,\n",
    "                         batch_size=batch_size,\n",
    "                         learning_rate_init=lr,\n",
    "                         max_iter=2000,\n",
    "                            verbose=False,\n",
    "                         )\n",
    "    model.fit(post_features,event_y)\n",
    "    if MODEL_USED== 'mlp':\n",
    "        loss.append(model.loss_)\n",
    "        print(\"Event {} loss : {} \".format(event,model.loss_))\n",
    "    # Save model\n",
    "    model_path = os.path.join(MODEL_DIR,MODEL_NAME+\"_\" +event+\".model\")\n",
    "    with open(model_path,\"wb\") as o:\n",
    "        pkl.dump(model,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1699/1699 [00:00<00:00, 312042.85it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 264166.52it/s]\n",
      "100%|██████████| 1699/1699 [00:00<00:00, 292653.90it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 278136.87it/s]\n",
      "100%|██████████| 1699/1699 [00:00<00:00, 301839.23it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 276395.65it/s]\n",
      "100%|██████████| 1699/1699 [00:00<00:00, 305370.35it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 286496.17it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 112147.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event NULL val accuracy : 0.84\n",
      "Event P001 val accuracy : 0.84\n",
      "Event P002 val accuracy : 1.00\n",
      "Event P003 val accuracy : 1.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PRED_DIR = 'pred/LF'\n",
    "MODE = 'LF'\n",
    "\n",
    "for event in EVENTS[:]:\n",
    "    # Load model for event and predict\n",
    "    model_path = os.path.join(MODEL_DIR,MODEL_NAME+\"_\" +event+\".model\")\n",
    "    with open(model_path,\"rb\") as l:\n",
    "        model = pkl.load(l,encoding='bytes')\n",
    "        val_pred = os.path.join(PRED_DIR,MODE+\"_val_\"+event+\".csv\")\n",
    "        test_pred = os.path.join(PRED_DIR,MODE+\"_\"+event+\".csv\")\n",
    "        \n",
    "        with open(test_pred,\"w\") as o: \n",
    "            test_soundnet_post_features = soundnet_classifier.predict_log_proba(test_soundnet_features)\n",
    "            test_resnet_post_features = resnet_classifier.predict_log_proba(test_resnet_features)\n",
    "            test_places_post_features = places_classifier.predict_log_proba(test_places_features)\n",
    "            \n",
    "            test_post_features = np.hstack((test_soundnet_post_features,test_resnet_post_features,test_places_post_features))\n",
    "            \n",
    "            prediction = model.predict_proba(test_post_features)\n",
    "            for p in tqdm(prediction):\n",
    "#                 print(p)\n",
    "                o.write(\"{}\\n\".format(p[1]))\n",
    "#           prediction = model.decision_function(feat)\n",
    "#           o.write(\"{}\\n\".format(prediction[0]))\n",
    "\n",
    "\n",
    "                    \n",
    "        with open(val_pred,\"w\") as o: \n",
    "            val_soundnet_post_features = soundnet_classifier.predict_log_proba(val_soundnet_features)\n",
    "            val_resnet_post_features = resnet_classifier.predict_log_proba(val_resnet_features)\n",
    "            val_places_post_features = places_classifier.predict_log_proba(val_places_features)\n",
    "            \n",
    "            val_post_features = np.hstack((val_soundnet_post_features,val_resnet_post_features,val_places_post_features))\n",
    "            \n",
    "            prediction = model.predict_proba(val_post_features)\n",
    "            for p in tqdm(prediction):\n",
    "                o.write(\"{}\\n\".format(p[1]))\n",
    "#               prediction = model.decision_function(feat)\n",
    "#               o.write(\"{}\\n\".format(prediction[0]))\n",
    "                     \n",
    "p0 = open(PRED_DIR + \"/\" + MODE + \"_val_NULL.csv\").readlines()                    \n",
    "p1 = open(PRED_DIR + \"/\" + MODE + \"_val_P001.csv\").readlines()\n",
    "p2 = open(PRED_DIR + \"/\" + MODE + \"_val_P002.csv\").readlines()\n",
    "p3 = open(PRED_DIR + \"/\" + MODE + \"_val_P003.csv\").readlines()\n",
    "\n",
    "p0 = [float(p.strip()) for p in p0]\n",
    "p1 = [float(p.strip()) for p in p1]\n",
    "p2 = [float(p.strip()) for p in p2]\n",
    "p3 = [float(p.strip()) for p in p3]\n",
    "\n",
    "acc = [0,0,0,0]\n",
    "total = [0,0,0,0]\n",
    "\n",
    "THRESHOLD = 0\n",
    "correct = 0\n",
    "with open(\"../all_val.lst\") as f:\n",
    "    lines = f.readlines()\n",
    "    for idx,line in enumerate(tqdm(lines)):\n",
    "        filename, truth = line.strip().split()\n",
    "        label = [p0[idx],p1[idx],p2[idx],p3[idx]]\n",
    "        pred = np.argmax(label)\n",
    "        \n",
    "        if EVENTS[pred] == truth:\n",
    "            acc[pred] += 1\n",
    "        total[EVENTS.index(truth)] += 1\n",
    "            \n",
    "acc = np.array(acc)/np.array(total)\n",
    "print(\"Event NULL val accuracy : {0:1.2f}\\nEvent P001 val accuracy : {0:1.2f}\\nEvent P002 val accuracy : {1:1.2f}\\nEvent P003 val accuracy : {2:1.2f}\\n\"\n",
    "      .format(acc[0],acc[1],acc[2],acc[3]))\n",
    "\n",
    "# Write params in log\n",
    "with open(\"logs.txt\",\"a\") as log:\n",
    "    if MODEL_USED == 'mlp':\n",
    "        log.write(\"{0}#{1}#{2}#{3}#{4}#{5:0.2f}#{6:0.2f}#{7:0.2f}#{8:0.2f}#{9:0.2f}#{10:0.2f}\\n\".format(MODEL_USED,hidden_layers,lr,batch_size,\n",
    "                                                       alpha,loss[0],loss[1],loss[2],acc[0],acc[1],acc[2]))\n",
    "    elif MODEL_USED == 'rf':\n",
    "        log.write(\"{0}#{1}#{2}#{3:0.2f}#{4:0.2f}#{5:0.2f}\\n\".format(MODEL_USED,num_estimators,max_depth,acc[0],acc[1],acc[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval.py list/P001_val_label pred/LF/LF_val_P001.csv\n",
    "!python eval.py list/P002_val_label pred/LF/LF_val_P002.csv\n",
    "!python eval.py list/P003_val_label pred/LF/LF_val_P003.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1699/1699 [00:00<00:00, 89532.03it/s]\n"
     ]
    }
   ],
   "source": [
    "p0 = open(PRED_DIR + \"/\" + MODE + \"_NULL.csv\").readlines()\n",
    "p1 = open(PRED_DIR + \"/\" + MODE + \"_P001.csv\").readlines()\n",
    "p2 = open(PRED_DIR + \"/\" + MODE + \"_P002.csv\").readlines()\n",
    "p3 = open(PRED_DIR + \"/\" + MODE + \"_P003.csv\").readlines()\n",
    "\n",
    "p0 = [float(p.strip()) for p in p0]\n",
    "p1 = [float(p.strip()) for p in p1]\n",
    "p2 = [float(p.strip()) for p in p2]\n",
    "p3 = [float(p.strip()) for p in p3]\n",
    "\n",
    "THRESHOLD = 0\n",
    "with open((MODE+\"_kaggle_prediction.csv\"),\"w\") as o:\n",
    "    o.write(\"VideoID,Label\\n\")\n",
    "    for idx,video in enumerate(tqdm(test_videos)):\n",
    "        label = [p0[idx],p1[idx],p2[idx],p3[idx]]\n",
    "        idx = np.argmax(label)\n",
    "        pred = idx\n",
    "        o.write(\"{},{}\\n\".format(video,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
