{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import _pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DIR = \"features\"\n",
    "EARLY_FUSION_DIR = \"features/early_fusion\"\n",
    "with open(\"list/all.video\") as f:\n",
    "    all_videos = [l.strip() for l in f.readlines()]\n",
    "\n",
    "with open('list/train.video') as f:\n",
    "    train_videos = [l.strip() for l in f.readlines()]\n",
    "    \n",
    "with open('list/val.video') as f:\n",
    "    val_videos = [l.strip() for l in f.readlines()]\n",
    "\n",
    "with open('../all_test.video') as f:\n",
    "    test_videos = [l.strip() for l in f.readlines()]\n",
    "    \n",
    "# train labels\n",
    "train_labels = {}\n",
    "with open(\"../all_trn.lst\") as f:\n",
    "    for line in f.readlines():\n",
    "        file, label = line.strip().split()\n",
    "        train_labels[file] = label\n",
    "        \n",
    "# val labels\n",
    "val_labels = {}\n",
    "with open(\"../all_val.lst\") as f:\n",
    "    for line in f.readlines():\n",
    "        file, label = line.strip().split()\n",
    "        val_labels[file] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and store early fusion features\n",
    "\n",
    "Early fusion features are generated by concatenating the SoundNet and Resnet50 features in to one single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(all_videos[:]):\n",
    "    soundnet_file = os.path.join(FEATURE_DIR,\"soundnet\",file+\".feats\")\n",
    "    resnet_file = os.path.join(FEATURE_DIR,\"resnet50\",file+\".npy\")\n",
    "    places_file = os.path.join(FEATURE_DIR,\"places\",file+\".npy\")\n",
    "    \n",
    "    with open(soundnet_file,\"rb\") as f:\n",
    "        soundnet_feature = np.loadtxt(f,delimiter=\";\")\n",
    "    \n",
    "    with open(resnet_file,\"rb\") as f:\n",
    "        resnet_feature = np.load(f)\n",
    "        \n",
    "    with open(places_file,\"rb\") as f:\n",
    "        places_feature = np.load(f)\n",
    "        \n",
    "    early_fusion = np.concatenate((soundnet_feature,resnet_feature,places_feature))\n",
    "    dest_path = os.path.join(EARLY_FUSION_DIR,file+\".npy\")\n",
    "    np.save(dest_path, early_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundnet_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and validation data for training classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "def load_data(video_list,labels_map=None):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for file in tqdm(video_list):\n",
    "        with open(os.path.join(EARLY_FUSION_DIR,file+\".npy\"),'rb') as f:\n",
    "            feature = np.load(f)\n",
    "            features.append(feature)\n",
    "            \n",
    "            if labels_map:\n",
    "                label = labels_map[file]\n",
    "                labels.append(label)\n",
    "            \n",
    "    features = np.vstack(features)\n",
    "    if labels_map:\n",
    "        return features,np.array(labels)\n",
    "    else:\n",
    "        return features\n",
    "    \n",
    "train_x, train_y = load_data(train_videos,train_labels)\n",
    "val_x, val_y = load_data(val_videos,val_labels)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample NULL class to equal that of events P00x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and validation set into a single train dataset\n",
    "combine = True\n",
    "if combine:\n",
    "    all_train_x = np.vstack([train_x,val_x])\n",
    "    all_train_y = np.concatenate((train_y,val_y))\n",
    "\n",
    "    # Shuffle dataset\n",
    "    idxs = [i for i in range(all_train_x.shape[0])]\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    all_train_x = all_train_x[idxs]\n",
    "    all_train_y = all_train_y[idxs]\n",
    "else:\n",
    "    all_train_x = train_x\n",
    "    all_train_y = train_y\n",
    "    \n",
    "positive_examples = all_train_x[all_train_y!='NULL']\n",
    "negative_examples = all_train_x[all_train_y=='NULL']\n",
    "\n",
    "positive_labels = all_train_y[all_train_y!='NULL']\n",
    "negative_labels = all_train_y[all_train_y == 'NULL']\n",
    "\n",
    "np.random.shuffle(negative_examples)\n",
    "negative_examples = negative_examples[:len(positive_labels)]\n",
    "negative_labels = negative_labels[:len(positive_labels)]\n",
    "\n",
    "combined_data = np.vstack((positive_examples,negative_examples))\n",
    "combined_labels = np.concatenate((positive_labels,negative_labels))\n",
    "\n",
    "# Shuffle the dataset\n",
    "idxs = [i for i in range(len(combined_data))]\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "combined_data = combined_data[idxs]\n",
    "combined_labels = combined_labels[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 3 separate models for each event\n",
    "# MLP parameters\n",
    "MODEL_USED = 'rf'\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_NAME = 'ef_exp1'\n",
    "EVENTS = [\"NULL\",\"P001\",\"P002\",\"P003\"]\n",
    "\n",
    "hidden_layers = (2048)\n",
    "lr = 1e-3\n",
    "batch_size = 150\n",
    "alpha = 1e-5\n",
    "loss = []\n",
    "\n",
    "\n",
    "# Random Forest params\n",
    "num_estimators = 500\n",
    "max_depth = 30\n",
    "\n",
    "for event in tqdm(EVENTS[:]):\n",
    "    # Train for event\n",
    "    event_y = (combined_labels==event).astype('int')\n",
    "    if MODEL_USED == 'rf':\n",
    "        model = RandomForestClassifier(n_estimators=num_estimators,max_depth=max_depth)\n",
    "#         model = GradientBoostingClassifier(n_estimators=num_estimators,max_depth=max_depth)\n",
    "    elif MODEL_USED == 'mlp':\n",
    "        model = MLPClassifier(hidden_layer_sizes=hidden_layers,\n",
    "                         alpha=alpha,\n",
    "                         batch_size=batch_size,\n",
    "                         learning_rate_init=lr,\n",
    "                         max_iter=2000,\n",
    "                            verbose=False,\n",
    "                         )\n",
    "    model.fit(combined_data,event_y)\n",
    "    if MODEL_USED== 'mlp':\n",
    "        loss.append(model.loss_)\n",
    "        print(\"Event {} loss : {} \".format(event,model.loss_))\n",
    "    # Save model\n",
    "    model_path = os.path.join(MODEL_DIR,MODEL_NAME+\"_\" +event+\".model\")\n",
    "    with open(model_path,\"wb\") as o:\n",
    "        pkl.dump(model,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_DIR = 'pred/EF'\n",
    "MODE = 'EF'\n",
    "\n",
    "for event in EVENTS[:]:\n",
    "    # Load model for event and predict\n",
    "    model_path = os.path.join(MODEL_DIR,MODEL_NAME+\"_\" +event+\".model\")\n",
    "    with open(model_path,\"rb\") as l:\n",
    "        model = pkl.load(l,encoding='bytes')\n",
    "        val_pred = os.path.join(PRED_DIR,MODE+\"_val_\"+event+\".csv\")\n",
    "        test_pred = os.path.join(PRED_DIR,MODE+\"_\"+event+\".csv\")\n",
    "        \n",
    "        with open(test_pred,\"w\") as o: \n",
    "            for video in tqdm(test_videos):\n",
    "                feat_path = os.path.join(EARLY_FUSION_DIR,video+\".npy\")\n",
    "                with open(feat_path,\"rb\") as f:\n",
    "                    feat  = np.load(f,encoding='bytes')\n",
    "                    feat = np.reshape(feat,(1,-1))\n",
    "#                     print(feat.shape)\n",
    "\n",
    "                    prediction = model.predict_proba(feat)\n",
    "                    o.write(\"{}\\n\".format(prediction[0][1]))\n",
    "#                     prediction = model.decision_function(feat)\n",
    "#                     o.write(\"{}\\n\".format(prediction[0]))\n",
    "\n",
    "\n",
    "                    \n",
    "        with open(val_pred,\"w\") as o: \n",
    "            for video in tqdm(val_videos):\n",
    "                feat_path = os.path.join(EARLY_FUSION_DIR,video+\".npy\")\n",
    "                with open(feat_path,\"rb\") as f:\n",
    "                    feat  = np.load(f,encoding='bytes')\n",
    "                    feat = np.reshape(feat,(1,-1))\n",
    "                    \n",
    "                    prediction = model.predict_proba(feat)\n",
    "                    o.write(\"{}\\n\".format(prediction[0][1]))\n",
    "#                     prediction = model.decision_function(feat)\n",
    "#                     o.write(\"{}\\n\".format(prediction[0]))\n",
    "                     \n",
    "p0 = open(PRED_DIR + \"/\" + MODE + \"_val_NULL.csv\").readlines()                    \n",
    "p1 = open(PRED_DIR + \"/\" + MODE + \"_val_P001.csv\").readlines()\n",
    "p2 = open(PRED_DIR + \"/\" + MODE + \"_val_P002.csv\").readlines()\n",
    "p3 = open(PRED_DIR + \"/\" + MODE + \"_val_P003.csv\").readlines()\n",
    "\n",
    "p0 = [float(p.strip()) for p in p0]\n",
    "p1 = [float(p.strip()) for p in p1]\n",
    "p2 = [float(p.strip()) for p in p2]\n",
    "p3 = [float(p.strip()) for p in p3]\n",
    "\n",
    "acc = [0,0,0,0]\n",
    "total = [0,0,0,0]\n",
    "\n",
    "THRESHOLD = 0\n",
    "correct = 0\n",
    "with open(\"../all_val.lst\") as f:\n",
    "    lines = f.readlines()\n",
    "    for idx,line in enumerate(tqdm(lines)):\n",
    "        filename, truth = line.strip().split()\n",
    "        label = [p0[idx],p1[idx],p2[idx],p3[idx]]\n",
    "        pred = np.argmax(label)\n",
    "        if EVENTS[pred] == truth:\n",
    "            acc[pred] += 1\n",
    "        total[EVENTS.index(truth)] += 1\n",
    "            \n",
    "acc = np.array(acc)/np.array(total)\n",
    "print(\"Event NULL val accuracy : {0:1.2f}\\nEvent P001 val accuracy : {0:1.2f}\\nEvent P002 val accuracy : {1:1.2f}\\nEvent P003 val accuracy : {2:1.2f}\\n\"\n",
    "      .format(acc[0],acc[1],acc[2],acc[3]))\n",
    "\n",
    "# Write params in log\n",
    "with open(\"logs.txt\",\"a\") as log:\n",
    "    if MODEL_USED == 'mlp':\n",
    "        log.write(\"{0}#{1}#{2}#{3}#{4}#{5:0.2f}#{6:0.2f}#{7:0.2f}#{8:0.2f}#{9:0.2f}#{10:0.2f}\\n\".format(MODEL_USED,hidden_layers,lr,batch_size,\n",
    "                                                       alpha,loss[0],loss[1],loss[2],acc[0],acc[1],acc[2]))\n",
    "    elif MODEL_USED == 'rf':\n",
    "        log.write(\"{0}#{1}#{2}#{3:0.2f}#{4:0.2f}#{5:0.2f}\\n\".format(MODEL_USED,num_estimators,max_depth,acc[0],acc[1],acc[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval.py list/P001_val_label pred/EF/EF_val_P001.csv\n",
    "!python eval.py list/P002_val_label pred/EF/EF_val_P002.csv\n",
    "!python eval.py list/P003_val_label pred/EF/EF_val_P003.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = open(PRED_DIR + \"/\" + MODE + \"_NULL.csv\").readlines()  \n",
    "p1 = open(PRED_DIR + \"/\" + MODE + \"_P001.csv\").readlines()\n",
    "p2 = open(PRED_DIR + \"/\" + MODE + \"_P002.csv\").readlines()\n",
    "p3 = open(PRED_DIR + \"/\" + MODE + \"_P003.csv\").readlines()\n",
    "\n",
    "p0 = [float(p.strip()) for p in p0]\n",
    "p1 = [float(p.strip()) for p in p1]\n",
    "p2 = [float(p.strip()) for p in p2]\n",
    "p3 = [float(p.strip()) for p in p3]\n",
    "\n",
    "THRESHOLD = 0\n",
    "with open((MODE+\"_kaggle_prediction.csv\"),\"w\") as o:\n",
    "    o.write(\"VideoID,Label\\n\")\n",
    "    for idx,video in enumerate(tqdm(test_videos)):\n",
    "        label = [p0[idx],p1[idx],p2[idx],p3[idx]]\n",
    "        idx = np.argmax(label)\n",
    "        \n",
    "        pred = idx\n",
    "            \n",
    "        o.write(\"{},{}\\n\".format(video,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
